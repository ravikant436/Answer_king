{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Odenet_hyper_hypo.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"E13VPct05Hcg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423229914,"user_tz":-120,"elapsed":10606,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["# Importe\n","from xml.etree import ElementTree as ET\n","import random\n","from random import *\n","import re\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import os\n","G=nx.Graph()\n","\n","# Ein- und Ausgabedateien: Achtung: OdeNet in der Version mit einem Eintrag pro Zeile, wenn man die Datei ändern will!\n","\n","#de_wn = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\odenet.git\\trunk\\deWordNet.xml\",\"r\",encoding=\"utf-8\")\n","#de_wn = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\deWNaccess\\odenet_oneline.xml\",\"r\",encoding=\"utf-8\")\n","#en_wn = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\English_WN\\english-wordnet-2020.xml\",\"r\",encoding=\"utf-8\")\n","\n","de_wn_file = \"/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/deWordNet.xml\"\n","de_wn = open(de_wn_file,\"r\",encoding=\"utf-8\")\n","\n","en_wn_file = \"/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/english_wn_xml/wn_god.xml\"\n","en_wn = open(en_wn_file,\"r\",encoding=\"utf-8\")\n","\n","#out_wn = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\deWNaccess\\odenet_oneline.xml\",\"w\",encoding=\"utf-8\")\n","\n","tree = ET.parse(de_wn)\n","\n","root = tree.getroot()\n","\n","lexicon = root.find('Lexicon')\n","\n","entree = ET.parse(en_wn)\n","\n","enroot = entree.getroot()\n","\n","enlexicon = enroot.find('Lexicon')\n","\n","de_wn.close()\n","en_wn.close()\n","\n","## Definitionen, die für die Methoden gebraucht werden\n","\n","# Alle vorhandenen Informationen zu einem Wort zugreifen\n","\n","def check_word_lemma(word_to_check):    \n","    for lexentry in lexicon.iter('LexicalEntry'):\n","        lemma = lexentry.find('Lemma')\n","        lemma_value = lemma.attrib['writtenForm']\n","        lemma_id = lexentry.attrib['id']\n","        if lemma_value == word_to_check:\n","            pos = lemma.attrib['partOfSpeech']\n","            senses = []\n","            for sense in lexentry.iter('Sense'):\n","                sense_id = sense.attrib['id']\n","                synset_id = sense.attrib['synset']\n","#                senserelation_type = lexentry.find('SenseRelation').attrib['relType']\n","#                senserelation_target = lexentry.find('SenseRelation').attrib['target']\n","                senses.append([sense_id,synset_id])\n","#            print(\"LEMMA: \" + lemma_value + \"\\nPOS: \" + pos + \"\\nSENSE ID: \" + sense_id)\n","            return(lemma_id, lemma_value, pos, senses)\n","\n","def words_in_synset(id):\n","    words = []\n","    for lexentry in lexicon.iter('LexicalEntry'):\n","        for sense in lexentry.iter('Sense'):\n","            if sense.attrib['synset'] == id:\n","                lemma = lexentry.find('Lemma').attrib['writtenForm']\n","                words.append(lemma)\n","    return(words)\n","\n","def en_words_in_synset(id):\n","    words = []\n","    for lexentry in enlexicon.iter('LexicalEntry'):\n","        for sense in lexentry.iter('Sense'):\n","            if sense.attrib['synset'] == id:\n","                lemma = lexentry.find('Lemma').attrib['writtenForm']\n","                words.append(lemma)\n","    return(words)\n","\n","def check_synset(id):\n","    words = words_in_synset(id)\n","    for synset in lexicon.iter('Synset'):\n","        if id == synset.attrib['id']:\n","            ili = synset.attrib['ili']\n","            try:\n","                en_definition = synset.attrib[\"{http://purl.org/dc/elements/1.1/}description\"]\n","            except KeyError:\n","                en_definition = []\n","            if synset.find('Definition') != None:\n","                de_definition = synset.find('Definition').text.strip()\n","            else:\n","                de_definition = []\n","            relations = []\n","            for relation in synset.iter('SynsetRelation'):\n","                reltype = relation.attrib['relType']\n","                reltarget = relation.attrib['target']\n","                relations.append((reltype,reltarget))\n","            return(ili,en_definition,de_definition, relations, words)\n","\n","def check_word_id(word_id):    \n","    for lexentry in lexicon.iter('LexicalEntry'):\n","        lemma = lexentry.find('Lemma')\n","        lemma_value = lemma.attrib['writtenForm']\n","        lemma_id = lexentry.attrib['id']\n","        if lemma_id == word_id:\n","            pos = lemma.attrib['partOfSpeech']\n","            senses = []\n","            for sense in lexentry.iter('Sense'):\n","                sense_id = sense.attrib['id']\n","                synset_id = sense.attrib['synset']\n","                senses.append([sense_id,synset_id])\n","            #print(\"LEMMA: \" + lemma_value + \"\\nPOS: \" + pos + \"\\nSENSE ID: \" + sense_id)\n","\n","def find_all_lexentries(word_to_check):\n","    for lexentry in lexicon.iter('LexicalEntry'):\n","        lemma = lexentry.find('Lemma')\n","        lemma_value = lemma.attrib['writtenForm']\n","        lemma_id = lexentry.attrib['id']\n","        word_id_list = []\n","        if lemma_value == word_to_check:\n","            word_id_list.append(lemma_id)\n","        for wid in word_id_list:\n","            check_word_id(wid)            \n","        \n","def hypernyms_word(word):\n","    lemma_id, lemma, pos, senses = check_word_lemma(word)\n","    hyp_list = []\n","    for sense in senses:\n","        (ili,definition,de_definition, relations, words) = check_synset(sense[1])\n","        for relation in relations:\n","            if relation[0] == \"hypernym\":\n","                hypernym_synset = relation[1]\n","                hypernym_words = words_in_synset(relation[1])\n","#            else:\n","#                hypernym_synset = []\n","#                hypernym_words = []               \n","                hyp_list.append((sense[0],hypernym_synset,hypernym_words))\n","    return(hyp_list)\n","\n","def hypernyms(sense):\n","    hyp_list = []\n","    (ili,definition,de_definition, relations, words) = check_synset(sense)\n","    for relation in relations:\n","        if relation[0] == \"hypernym\":\n","            hypernym_synset = relation[1]\n","            hypernym_words = words_in_synset(relation[1])\n","            hyp_list.append((hypernym_synset,hypernym_words))\n","    return(hyp_list)\n","\n","def hyponyms(sense):\n","    hyp_list = []\n","    (ili,definition,de_definition, relations, words) = check_synset(sense)\n","    for relation in relations:\n","        if relation[0] == \"hyponym\":\n","            hyponym_synset = relation[1]\n","            hyponym_words = words_in_synset(relation[1])\n","            hyp_list.append((hyponym_synset,hyponym_words))\n","    return(hyp_list)\n","\n","def hyponyms_word(word):\n","    lemma_id, lemma, pos, senses = check_word_lemma(word)\n","    hyp_list = []\n","    for sense in senses:\n","        (ili,definition,de_definition, relations, words) = check_synset(sense[1])\n","        for relation in relations:\n","            if relation[0] == \"hyponym\":\n","                hyponym_synset = relation[1]\n","                hyponym_words = words_in_synset(relation[1])\n"," #           else:\n"," #               hyponym_synset = []\n"," #               hyponym_words = []               \n","                hyp_list.append((sense[0],hyponym_synset,hyponym_words))\n","    return(hyp_list)\n","\n","def meronyms_word(word):\n","    lemma_id, lemma, pos, senses = check_word_lemma(word)\n","    mero_list = []\n","    for sense in senses:\n","        (ili,definition,de_definition, relations, words) = check_synset(sense[1])\n","        for relation in relations:\n","            if relation[0] == \"mero_part\":\n","                meronym_synset = relation[1]\n","                meronym_words = words_in_synset(relation[1])\n","                mero_list.append((sense[0],meronym_synset,meronym_words))\n","    return(mero_list)\n","\n","def holonyms_word(word):\n","    lemma_id, lemma, pos, senses = check_word_lemma(word)\n","    holo_list = []\n","    for sense in senses:\n","        (ili,definition,de_definition, relations, words) = check_synset(sense[1])\n","        for relation in relations:\n","            if relation[0] == \"holo_part\" or relation[0] == \"holo_member\":\n","                holo_synset = relation[1]\n","                holo_words = words_in_synset(relation[1])\n","                holo_list.append((sense[0],holo_synset,holo_words))\n","    return(holo_list)\n","\n","def antonyms_word(word):\n","    lemma_id, lemma, pos, senses = check_word_lemma(word)\n","    anto_list = []\n","    for sense in senses:\n","        (ili,definition,de_definition, relations, words) = check_synset(sense[1])\n","        for relation in relations:\n","            if relation[0] == \"antonym\":\n","                antonym_synset = relation[1]\n","                antonym_words = words_in_synset(relation[1])\n","                anto_list.append((sense[0],antonym_synset,antonym_words))\n","    return(anto_list)\n","\n","def words2ids(wordlist):\n","    word_id_list = []\n","    for word in wordlist:\n","        try:\n","            lemma_id, lemma, pos, senses = check_word_lemma(word)\n","            word_id_list.append(lemma_id)\n","        except:\n","            print(word + \" NOT IN ODENET\")\n","    return(word_id_list)\n","    \n","\n","\n","# Visualisierungen\n","\n","seen = set()\n","\n","def recurse_hyper(s,word):\n","    if not s in seen:\n","        seen.add(s)\n","        G.add_node(word)\n","        hypers = hypernyms(s)\n","        print(str(hypers))\n","#        if len(hypers) > 0:\n","        for h in hypers:\n","            G.add_node(h[1][0])\n","            G.add_edge(word,h[1][0])\n","            recurse_hyper(h[0],h[1][0])\n","\n","def recurse_hypo(s,word):\n","    if not s in seen:\n","        seen.add(s)\n","        G.add_node(word)\n","        hypos = hyponyms(s)\n","        for h in hypos:\n","            G.add_node(h[1][0])\n","            G.add_edge(word,h[1][0])\n","            recurse_hypo(h[0],h[1][0])\n","\n","def recurse(s,word):\n","    if not s in seen:\n","        seen.add(s)\n","        G.add_node(word)\n","        hypers = hypernyms(s)\n","        hypos = hyponyms(s)\n","        print(str(hypers))\n","        print(str(hypos))\n","#        if len(hypers) > 0:\n","        for h in hypers:\n","            G.add_node(h[1][0])\n","            G.add_edge(word,h[1][0])\n","            recurse_hyper(h[0],h[1][0])\n","        for h in hypos:\n","            G.add_node(h[1][0])\n","            G.add_edge(word,h[1][0])\n","            recurse_hypo(h[0],h[1][0])\n","\n","#recurse_hyper(sense,word)\n","\n","#recurse_hypo(sense,word)\n","\n","def visualize_hypernyms():\n","    word = input(\"Welches Wort? \")\n","    lemma, pos, senses = check_word_lemma(word)\n","    print(\"Ich habe diese Senses dafür: \")\n","    for s in senses:\n","        print(\"SYNSET: \" + s[1])\n","        print(check_synset(s[1]))\n","    sense = input(\"Welcher Sense? \")      \n","    recurse_hyper(sense,word)\n","    print (G.nodes(data=True))\n","    nx.draw_networkx(G, width=2, with_labels=True)\n","    plt.show()\n","\n","def visualize_hyponyms():\n","    word = input(\"Welches Wort? \")\n","    lemma, pos, senses = check_word_lemma(word)\n","    print(\"Ich habe diese Senses dafür: \")\n","    for s in senses:\n","        print(\"SYNSET: \" + s[1])\n","        print(check_synset(s[1]))\n","    sense = input(\"Welcher Sense? \")\n","    recurse_hypo(sense,word)\n","    print (G.nodes(data=True))\n","    nx.draw_networkx(G, width=2, with_labels=True)\n","    plt.show()\n","\n","\n","               \n","## Die Klasse für den Zugriff auf OdeNet\n","\n","class OdeNet(object):\n","    def word_info(object):\n","        if check_word_lemma(object):\n","          (lemma_id, lemma_value, pos, senses) = check_word_lemma(object)\n","          # print (lemma_value + \" \" + pos + \" \")\n","          # for sense in senses:\n","          #     print(\"SENSE: \" + str(sense[1]) + \"  \" + str(check_synset(sense[1])) + \"\\n\")\n","          # print(\"HYPERNYMS: \" + str(hypernyms_word(object)))\n","          # print(\"HYPONYMS: \" + str(hyponyms_word(object)))\n","          # print(\"MERONYMS: \" + str(meronyms_word(object)))\n","          # print(\"HOLONYMS: \" + str(holonyms_word(object)))\n","          # print(\"ANTONYMS: \" + str(antonyms_word(object)))\n","          find_all_lexentries(object) \n","          return [senses, hypernyms_word(object), hyponyms_word(object)]\n","        else:\n","          return \"\"                   \n","    pass\n","    def check_ili_in_pwn(object):\n","        for lexentry in enlexicon.iter('LexicalEntry'):\n","            lemma = lexentry.find('Lemma')\n","            lemma_value = lemma.attrib['writtenForm']\n","            if lemma_value == object:\n","                pos = lemma.attrib['partOfSpeech']\n","                senses = []\n","                for sense in lexentry.iter('Sense'):\n","                    synset_id = sense.attrib['synset']\n","                    senses.append(synset_id)\n","                for sense in senses:\n","                    words = en_words_in_synset(sense)\n","                    print(str(words))\n","                    for synset in enlexicon.iter('Synset'):\n","                        if sense == synset.attrib['id']:\n","                            ili = synset.attrib['ili']\n","                            print(str(ili))\n","                            definition = synset.find('Definition').text.strip()\n","                            print(str(definition))\n","    pass\n","    def word_id(object):\n","        lemma_id, lemma_value, pos, senses = check_word_lemma(object)\n","        return lemma_id\n","    pass\n","    def visualize(object):\n","        lemma_id, lemma_value, pos, senses = check_word_lemma(object)\n","        if len(senses) == 1:\n","            sense = senses[0][1]\n","        else:\n","            print(\"Ich habe diese Senses dafür: \")\n","            for s in senses:\n","                print(\"SYNSET: \" + s[1])\n","                print(check_synset(s[1]))\n","            sense = input(\"Welcher Sense? \")      \n","        recurse(sense,object)\n","        print (G.nodes(data=True))\n","        nx.draw_networkx(G, width=2, with_labels=True)\n","        plt.show()\n","    pass\n","\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_c2ixlLCZHM","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423234142,"user_tz":-120,"elapsed":485,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["# Importe\n","\n","\n","## Funktionen für die Arbeit mit OdeNet\n","\n","# Informationen zu OdeNet hinzufügen\n","# nur in oneline-Datei!\n","\n","# Relationen dem Synset hinzufügen\n","# add_rel_to_ss(synset,relation,r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\deWNaccess\\odenet_oneline.xml\")\n","\n","\n","def add_rel_to_ss(synset,relation,wordnetfile):\n","    if synset not in relation:\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close() \n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        ss_string = '<Synset id=\"' + synset + '\"'\n","        for line in lines:\n","            if ss_string in line and relation not in line:\n","                if '<Example>' in line:\n","                    line = line.replace('<Example>',relation + '<Example>')\n","                elif '</Synset>' in line:\n","                    line = line.replace('</Synset>',relation + '</Synset>')\n","                else:\n","                    line = line.replace('/>', '>' + relation + '</Synset>')\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","        \n","# Attribute in Synsets verändern, z.B. ili\n","# change_attribute_in_ss('odenet-412-a','ili','i10007',r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\deWNaccess\\odenet_oneline.xml\")\n","\n","def change_attribute_in_ss(synset,att,value,wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        ss_string = '<Synset id=\"' + synset + '\"'\n","        for line in lines:\n","            if ss_string in line:\n","                line = re.sub(att + '=\"[a-zA-Z0-9]*\"', att + '=\"'+ value +'\"', line)\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","\n","# Attribute zu Lexentries hinzufügen, z.B. confidenceScore\n","\n","def change_attribute_in_lexentry(lemma,att,value,wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        lemma_string = 'Lemma writtenForm=\"' + lemma + '\"'\n","        for line in lines:\n","            if lemma_string in line and att not in line:\n","                line = line.replace(\"><Lemma writtenForm=\",' ' + att + '=\"'+ value + '\"' + \"><Lemma writtenForm=\")\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","    \n","# Attribute zu Senses hinzufügen, z.B. note\n","# Sense id=\"w18234_4118-n\" synset=\"odenet-4118-n\" note=\"PHON:boːt\"/>\n","\n","def add_attribute_to_sense(sense_id,synset,att,value,wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        sense_string = 'Sense id=\"' + sense_id + '\" synset=\"'+ synset + '\"'\n","        for line in lines:\n","            if sense_string in line and value not in line:\n","#            if sense_string in line:\n","                line = line.replace(sense_string,sense_string + ' ' + att + '=\"'+ value + '\"')\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","        \n","\n","    \n","# Definitionen zu einem Synset hinzufügen    \n","def add_definition_to_ss(synset,definition, wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        ss_string = '<Synset id=\"' + synset + '\"'\n","        definition_string = \"<Definition>\" + definition + \"</Definition>\"\n","        for line in lines:\n","            if ss_string in line and \"<Definition>\" not in line:\n","                if '<Example>' in line:\n","                    line = line.replace('<Example>', definition_string + '<Example>')\n","                elif '<SynsetRelation' in line:\n","                    line = line.replace('<SynsetRelation', definition_string + '<SynsetRelation',1)\n","                elif '</Synset>' in line:\n","                    line = line.replace('</Synset>', definition_string + '</Synset>')\n","                else:\n","                    line = line.replace('/>', '>' + definition_string + '</Synset>')\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","\n","# Die englische Definition löschen\n","def delete_english_definition(synset, wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        ss_string = '<Synset id=\"' + synset + '\"'\n","        for line in lines:\n","            if ss_string in line and \"dc:description\" in line:\n","                line=re.sub('dc:description=\".*\"','',line)\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","\n","# Ein Beispiel einfügen\n","\n","def add_example_to_ss(synset,example, wordnetfile):\n","        de_wn = open(wordnetfile,\"r\",encoding=\"utf-8\")\n","        lines = de_wn.readlines()\n","        de_wn.close()\n","        out_odenet = open(wordnetfile,\"w\",encoding=\"utf-8\")\n","        ss_string = '<Synset id=\"' + synset + '\"'\n","        example_string = \"<Example>\" + example + \"</Example>\"\n","        for line in lines:\n","            if ss_string in line and \"<Example>\" not in line:\n","                if '</Synset>' in line:\n","                    line = line.replace('</Synset>', example_string + '</Synset>')\n","                print(line)\n","            out_odenet.write(line)\n","        out_odenet.close()\n","\n","# Die Version ohne Zeilenumbruch als Pretty Print speichern\n","\n","def prettyprint_odenet():\n","    oneline_odenet = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\deWNaccess\\odenet_oneline.xml\",\"r\", encoding=\"utf-8\")\n","    lines = oneline_odenet.readlines()\n","    oneline_odenet.close()\n","    pretty_odenet = open(r\"C:\\Users\\melaniesiegel\\Documents\\05_Projekte\\WordNet\\OdeNet\\odenet.git\\trunk\\deWordNet.xml\",\"w\",encoding=\"utf-8\")\n","    for line in lines:\n","        line = line.replace(\"<Lemma\",\"\\n\\t<Lemma\")\n","        line = line.replace(\"<Sense\",\"\\n\\t<Sense\")\n","        line = line.replace(\"</Sense\",\"\\n\\t</Sense\")\n","        line = line.replace(\"</LexicalEntry>\",\"\\n</LexicalEntry>\")\n","        line = line.replace(\"<SynsetRelation\",\"\\n\\t<SynsetRelation\")\n","        line = line.replace(\"<Definition>\",\"\\n\\t<Definition>\")\n","        line = line.replace(\"<Example>\",\"\\n\\t<Example>\")\n","        line = line.replace(\"</Synset>\",\"\\n</Synset>\")\n","        line = line.replace(\"<SyntacticBehaviour\",\"\\n\\t<SyntacticBehaviour\")\n","        pretty_odenet.write(line)\n","    pretty_odenet.close()\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"BYXWU3mJ0QhO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423236152,"user_tz":-120,"elapsed":640,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["#OdeNet.word_info(\"Auseinandersetzung\")\n","#OdeNet.word_info(\"Verp\")\n","# senses, hyper, hypo"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvgJZZcs0QhU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423238427,"user_tz":-120,"elapsed":647,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["#OdeNet.check_ili_in_pwn(\"show\")"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"isIEFiNE0Qh1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423239098,"user_tz":-120,"elapsed":429,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["# Hyponyme und Hyperonyme visualisieren\n","#OdeNet.visualize(\"Auseinandersetzung\")\n","#OdeNet.visualize(\"Fall\")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6acQq8ARwrd-","colab_type":"text"},"source":["#Text Lemmatization using Tiger Corpus\n","\n","Source: https://github.com/WZBSocialScienceCenter/germalemma"]},{"cell_type":"code","metadata":{"id":"5-mhfk_e9TB3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1595423248346,"user_tz":-120,"elapsed":8217,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"b515ddef-a3cc-4f01-85b0-b89504d47b87"},"source":["pip install -U germalemma"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Collecting germalemma\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/94/b335fa67ec8dd6fca977769c17f657526f66e0bcb3a10f44f890ea16555a/germalemma-0.1.3-py3-none-any.whl (2.3MB)\n","\u001b[K     |████████████████████████████████| 2.3MB 3.9MB/s \n","\u001b[?25hCollecting PatternLite>=3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/f5/1894eb24102cae0e433c18366ec2a8d945b42cbf128303b67454db8587d8/PatternLite-3.6-py3-none-any.whl (22.1MB)\n","\u001b[K     |████████████████████████████████| 22.1MB 62.6MB/s \n","\u001b[?25hCollecting Pyphen>=0.9.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 48.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from PatternLite>=3.6->germalemma) (1.18.5)\n","Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from PatternLite>=3.6->germalemma) (1.4.1)\n","Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from PatternLite>=3.6->germalemma) (3.2.5)\n","Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->PatternLite>=3.6->germalemma) (1.12.0)\n","Installing collected packages: PatternLite, Pyphen, germalemma\n","Successfully installed PatternLite-3.6 Pyphen-0.9.5 germalemma-0.1.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cLcJaMyvwyJD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423252567,"user_tz":-120,"elapsed":1481,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["from germalemma import GermaLemma\n","\n","lemmatizer = GermaLemma()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9Aavx3WxIH7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595423253991,"user_tz":-120,"elapsed":610,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"4609b543-7925-4c94-dacc-0962f9bafd48"},"source":["# passing the word and the POS tag (\"N\" for noun)\n","lemma = lemmatizer.find_lemma('Feinstaubbelastungen', 'N')\n","print(lemma)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Feinstaubbelastung\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"978sCSzR7BDI","colab_type":"text"},"source":["#Get POS of the words\n","\n","We are using Spacy"]},{"cell_type":"code","metadata":{"id":"L-AIP8OD8EGf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"status":"ok","timestamp":1595423265834,"user_tz":-120,"elapsed":9271,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"757d5c4b-f75c-4eac-9b85-bcc734bb3710"},"source":["pip install --upgrade spacy"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Collecting spacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n","\u001b[K     |████████████████████████████████| 10.0MB 4.3MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.1.0)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.0)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Collecting thinc==7.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 47.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n","Installing collected packages: thinc, spacy\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed spacy-2.3.2 thinc-7.4.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BQPGFZPZSKow","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":574},"executionInfo":{"status":"ok","timestamp":1595423285119,"user_tz":-120,"elapsed":15460,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"dbc1ebdc-62c0-4ffc-8bb5-9b8986b9c6c5"},"source":["!python -m spacy download de_core_news_md"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Collecting de_core_news_md==2.3.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_md-2.3.0/de_core_news_md-2.3.0.tar.gz (47.0MB)\n","\u001b[K     |████████████████████████████████| 47.0MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from de_core_news_md==2.3.0) (2.3.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (49.1.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (3.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.1.3)\n","Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (7.4.1)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (4.41.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (0.7.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (2.0.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.18.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.0.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.7.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->de_core_news_md==2.3.0) (3.1.0)\n","Building wheels for collected packages: de-core-news-md\n","  Building wheel for de-core-news-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for de-core-news-md: filename=de_core_news_md-2.3.0-cp36-none-any.whl size=47054219 sha256=9c12ffb23d2f9c444a51cec01fb31925de7478a09e9ffd126d629bb3fd1ed623\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-hta0_8r2/wheels/61/d0/4e/98754e2456d0650faecf781724db11f925c1610653914e6ee4\n","Successfully built de-core-news-md\n","Installing collected packages: de-core-news-md\n","Successfully installed de-core-news-md-2.3.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('de_core_news_md')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sTp30fVscVQ5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423287686,"user_tz":-120,"elapsed":636,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["import spacy\n","import de_core_news_md"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"KluzM9Q-cXeq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423292891,"user_tz":-120,"elapsed":4265,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["nlp = de_core_news_md.load()"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"03sg8uhfSjml","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423377302,"user_tz":-120,"elapsed":688,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["def getPos(word):\n","  text = (word) \n","  doc = nlp(text)\n","  for token in doc: \n","    if token.pos_ == \"NOUN\" or token.pos_ == \"PROPN\" or token.pos_ == \"X\":\n","      return 'N'\n","    elif token.pos_ == \"VERB\":\n","      return 'V'\n","    elif token.pos_ == \"ADJ\" or token.pos_ == \"ADV\":\n","      return token.pos_\n","    else:\n","      return 'N'"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"qagIUm06oaQi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423295550,"user_tz":-120,"elapsed":861,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["def getLemma(word):\n","  word_pos = getPos(\"\" + word + \"\")\n","  lemma = lemmatizer.find_lemma(word, word_pos)\n","  return [word, lemma, word_pos]"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"lO2OHQVvpxFm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1595423296900,"user_tz":-120,"elapsed":571,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"b99a71f5-aa6e-4504-8e59-4b1370900b68"},"source":["print(getLemma(\"Forstwirten\"))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["['Forstwirten', 'Forstwirten', 'N']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OwaN0HJvqPjV","colab_type":"text"},"source":["#Process Words"]},{"cell_type":"code","metadata":{"id":"ufBDKH0wqPFJ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423300231,"user_tz":-120,"elapsed":612,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["manual_words_list = ['Buchführung',  'Rechnungslegung',  'Buchführung',  'Verwaltungsanweisung',  'Freibetrags',  'Rechtsbehelfsverfahrens ',  'Anwendung',  'Veranlagungszeitraum',  'Ausgangswertgrenze',  'Bayern',  'Buchwert',  'Betriebsvermögen',  'Vermögen ',  'Fall',  'Bescheinigung',  'Oberfinanzdirektion',  'Kirchensteuer',  'Rundverfügung',  'Köln',  'Handelsrecht',  'Gemeind',  'Ausgleichsanspruch ',  'Zustaendigkeit',  'Beschwerd',  'Lohnunternehm',  'Gesellschaftsrecht',  'Ausschlussfrist',  'Ausschlußfrist',  'Schulden',  'Unterlage',  'Düsseldorf',  'Arbeitsvertrag',  'Erklärung',  'Betriebsinhaber',  'Land- und Forstwirte',  'Bundesfinanzhof',  'Frankfurt am Main',  'Förderungsgesetze',  'Allgemeines ',  'Lohnsteuer',  'Einkommensteuerveranlagung',  'Versicherung',  'Zinseinnahmen',  'Schuldzinsen',  'Einführungsschreiben',  'Urteil',  'Zuständigkeit ',  'Kiel',  'Kiel',  'Koblenz',  'Koblenz',  'Gesetz',  'Pachtung',  'Verpachten',  'Pachterlöse',  'Verpachtungserlaß',  'Vermächtnisnehmer',  'Verpächter',  'Viehdurchschnittswerte',  'Bewirtschafter',  'Wirtschaftsjahrs',  'Wirtschaftsjahre',  'Mitteilung',  'Münster',  'Reingewinn',  'Nordrhein-Westfalen',  'Nürnberg',  'Nürnberg',  'Eröffnungsbilanz',  'Betriebsausgaben',  'Betriebseinnahmen',  'Betriebsaufgabe',  'Absatz',  'Teilwerts',  'Teilwert',  'Zahlung',  'Geltungsdauer',  'Privatvermögen',  'Gewinngrenze ',  'Nachweis',  'Eigentum',  'Vermögensgrenze ',  'Bestimmungen',  'Bezugsverfügung',  'Aufzeichnungen ',  'Tilgung',  'Vorschrift',  'Aufforderung',  'Einnahmen',  'Wohnrecht',  'Saarbrücken',  'Saarbrücken',  'Saarland',  'Umsatz',  'Umsätze',  'Handelsvertreters',  'Abfindungen',  'Gesellschaftsvertrages',  'Alleinerben',  'Ehegatt',  'Ehegatte',  'Ausgangsbetrag',  'Erbfolge',  'Zuschlag ',  'Übernahme ',  'Steuerbescheid',  'Abgabenordnung',  'Steuerhinterziehung',  'Steuergesetz',  'Finanzamts ',  'Steuervergünstigung',  'Steuerbegünstigung',  'Besteuerung ',  'Steuerpflichtiger',  'Steuerpflichtigen',  'Pächter', 'Nutzungswert',  'Entnahmewertes',  'Arbeitskraft']"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"z6btq-HQp3RL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423301208,"user_tz":-120,"elapsed":413,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["algo_10_files_word_list = ['Abfindung', 'Abs', 'Abschnitt', 'AfA', 'Allgemeines', 'Anbau',\n","       'Anhalt', 'Anlage', 'Antrags', 'Arbeitsleistung', 'Berechnung',\n","       'Betracht', 'Betrieb', 'Betriebs', 'Betriebsausgaben',\n","       'Betriebsinhaber', 'Betriebsvermögen', 'Bewirtschaftung', 'Bodens',\n","       'Buchführung', 'Buchführungspflicht', 'Buchstabe', 'EStG',\n","       'Ehegatten', 'Einkünfte', 'Einkünften', 'Erben', 'Erfassung',\n","       'Erhöhte', 'Erlasses', 'Ermittlung', 'Festsetzung', 'Feststellung',\n","       'Feststellungen', 'Finanzamt', 'Fläche', 'Flächen',\n","       'Forstwirtschaft', 'Fragebogen', 'Freibetrag', 'Freibetrags',\n","       'Gegenüberstellung', 'Gewinn', 'Gewinne', 'Gewinns', 'Gewährung',\n","       'Grundstück', 'Grundstücke', 'Grundstücken', 'Herstellungskosten',\n","       'III', 'Inventar', 'Land-', 'Landwirt', 'Nachweis', 'Nutzungen',\n","       'Obstbau', 'Privatvermögen', 'Schuldzinsen', 'Schätzung',\n","       'Sonderabschreibungen', 'Sonderausgaben', 'Steuerpflichtigen',\n","       'Tieren', 'Tierhaltung', 'Tilgung', 'Veranlagung', 'Veräußerung',\n","       'Veräußerungen', 'Wahlrecht', 'Weinbau', 'Werts',\n","       'Wirtschaftsgüter', 'Wirtschaftsgütern', 'Wirtschaftsjahr',\n","       'Wohnung', 'Wurde', 'Ziffer', 'Zuschlag', 'Zuschläge', 'abgesetzt',\n","       'abweicht', 'angesetzten', 'anzusetzen', 'anzuwenden',\n","       'aufgeführten', 'ausgewiesenen', 'bekanntgegeben', 'beschäftigten',\n","       'bewirtschaftet', 'bezeichneten', 'dauernde', 'enthaltenen',\n","       'entnommen', 'erfaßt', 'erhöhte', 'erhöhten', 'ermitteln',\n","       'gehörende', 'gehörenden', 'gem', 'genutzte', 'gärtnerische',\n","       'körperliche', 'land-', 'landwirtschaftlich',\n","       'landwirtschaftliche', 'landwirtschaftlichen',\n","       'landwirtschaftlicher', 'maßgebend', 'maßgebende', 'maßgebenden',\n","       'nachgewiesenen', 'sachlichen', 'sinngemäß', 'unentgeltlich',\n","       'veräußert', 'vgl', 'zuzurechnen', 'Überlassung', 'überführen']"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1BK71retvua","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423302485,"user_tz":-120,"elapsed":590,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["tag_me_10_files_word_list = ['Abschreibung', 'Absetzung für Abnutzung', 'Altenteil', 'Anlagevermögen', 'Anschaffungskosten', 'Arbeitsrecht', 'Arnsberg', 'Aßberg', 'Ausschlussfrist', 'Außenprüfung', 'Ayten Erten', 'Baumartengruppe', 'Before Present', 'Berufsgenossenschaft', 'Betriebsausgabe', 'Betriebsfinanzamt', 'Betriebsveräußerung', 'Betriebsvermögen', 'Betriebsvermögensvergleich', 'Bodenschätzung', 'Bohne', 'Branntwein', 'Buchführung', 'Bundesautobahn', 'Bundesautobahn ', 'Bundesautobahn 6', 'Bundesfinanzhof', 'Bundesministerium der Finanzen', 'Darfur', 'Dauernde Last', 'Deich', 'Echter Sellerie', 'Einheitswert', 'Einkommensteuer', 'Einkommensteuer (Deutschland)', 'Einkommensteuer-Durchführungsverordnung', 'Einkommensteuer-Richtlinien', 'Einkommensteuererklärung', 'Einkommensteuergesetz', 'Einkommensteuergesetzbuch', 'Einkünfte aus Gewerbebetrieb (Deutschland)', 'Einkünfte aus nichtselbständiger Arbeit (Deutschland)', 'Emanuele Vona', 'Emrah Eren', 'Endivie', 'Enkirch', 'Erbbaurecht', 'Erbbauzins', 'Erbschaftsteuer', 'Erbse', 'Erf (Fluss)', 'Errungenschaftsgemeinschaft', 'Erté', 'Ertrag', 'Esparsetten', 'Ester', 'Fahrnisgemeinschaft', 'Feldsalat', 'Finanzamt', 'Flurbereinigung', 'Forstwirtschaft', 'Fortgesetzte Gütergemeinschaft', 'Freinsheim', 'Friedelsheim', 'Fruchtfolge', 'Gabelstapler', 'Gebäudeversicherung', 'Gemeiner Wert', 'Gemüse', 'Gemüsebau', 'Gemüsespargel', 'Gerste', 'Geschäftsjahr', 'Gesetzliche Erbfolge', 'Getreide', 'Gewinn', 'Gönnheim', 'Grafschaft Saarbrücken', 'Grundlagenbescheid', 'Grundsteuer', 'Grundsteuer (Deutschland)', 'Grünland', 'Gurke', 'Gütergemeinschaft (Ehe)', 'Güterrecht', 'Hackfrucht', 'Hafer', 'Handelsgesetzbuch', 'Handelsregister', 'Haushaltsjahr', 'Hefen', 'Herstellungskosten', 'Hofladen', 'Hopfen', 'Hypothekengewinnabgabe', 'Imker', 'Immaterieller Vermögensgegenstand', 'Infimum und Supremum', 'Kalamität', 'Kallstadt', 'Karotte', 'Kataster', 'Kaufmann', 'Kinderfreibetrag', 'Kleegras', 'Kleingarten', 'Kleingewerbe', 'Koblenz', 'Kohl', 'Kohlrabi', 'Kopfkohl', 'Kopfsalat', 'Korken', 'Kostenmiete', 'Kreis Brilon', 'Kreis Olpe', 'Land- und forstwirtschaftlicher Betrieb', 'Landwirtschaftliche Alterskasse', 'Lauch', 'Laufendes Gut', 'Leistadt', 'Lohnsteuer', 'Lohnsteuer (Deutschland)', 'Lohnsteueraußenprüfung', 'Lohnunternehmer', 'Lüdenscheid', 'Lupinen', 'Luzerne', 'Maischen', 'Meschede', 'Mietspiegel', 'Minderung der Erwerbsfähigkeit', 'Mitunternehmerschaft', 'Mosel (Weinbaugebiet)', 'Naci Erdem', 'Nießbrauch', 'Nürnberg', 'Nutzungswert', 'Oberfinanzdirektion', 'Oberfinanzdirektion Münster', 'Obstbau', 'Orhan Eren', 'Pilzanbau', 'Reblaus', 'Rettiche', 'Roggen', 'Rote Bete', 'Rotkohl', 'Rudolf Felzmann (Heimatforscher)', 'Ruwer', 'Saatzucht', 'Shinyanga (Region)', 'Sonderabschreibung', 'Sonderkultur', 'Sondernutzung', 'Sozialbrache', 'Speyer', 'Spinat', 'ST 16', 'Ständigkeit', 'Stattgabe', 'Steuerbescheid', 'Steuererklärung', 'Steuerermäßigung', 'Steuergefährdung', 'Steuerhinterziehung (Deutschland)', 'Steuersubvention', 'Stille Reserven', 'Teichwirtschaft', 'Teilwert', 'Tomate', 'Trester (Pressrückstände)', 'Umsatzsteuer', 'Umsatzsteuergesetz', 'Ungstein', 'Veranlagung (Steuerrecht)', 'Veranlagungszeitraum', 'Veräußerungsgewinn', 'Verfahrensrecht', 'Vergleichsmiete', 'Vermögensteuer', 'Vertova', 'Verwaltungsrecht (Deutschland)', 'Vorbehalt der Nachprüfung', 'Vorsteuerpauschale', 'Weinbau', 'Weinherstellung', 'Weißkohl', 'Weizen', 'Wicken (Vicia)', 'Wirsing', 'Wirtschaftliche Betrachtungsweise', 'Wirtschaftsgut (Steuerlehre)', 'Zierpflanzenbau', 'Zwiebel']"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sYzJ6U0OueTu","colab_type":"text"},"source":["**Process Manual words**"]},{"cell_type":"code","metadata":{"id":"eTjU2ge-vuMx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423304732,"user_tz":-120,"elapsed":573,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["import pandas as pd"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qjRxcbSk9fQl","colab_type":"text"},"source":["**Clean Tag-me words**"]},{"cell_type":"code","metadata":{"id":"AgsArc7r_9Dj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1595423307828,"user_tz":-120,"elapsed":1311,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"47f7eef0-d4df-4949-8337-07b48a2c612e"},"source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"K4U5V-6uADkD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423311870,"user_tz":-120,"elapsed":455,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["german_stop_words = stopwords.words('german')"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"gCRbvgbr-87d","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423316808,"user_tz":-120,"elapsed":540,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["tag_me_word_list = []\n","for word in tag_me_10_files_word_list:\n","  if(len(word.split(' '))) == 1:\n","    tag_me_word_list.append(word)\n","  else:\n","    word_arr = word.split(' ')\n","    for item in word_arr:\n","      if item not in german_stop_words and item != '' and not item.isnumeric() and not item.startswith('('):\n","        tag_me_word_list.append(item)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkGwPYUZGSje","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423317944,"user_tz":-120,"elapsed":499,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["def lemmatize_pos_wordlist(wordList):\n","  temp_list = []\n","  for word in wordList:\n","    temp_l = getLemma(word)\n","    temp_list.append(temp_l)\n","  words_df = pd.DataFrame(temp_list, columns=['Raw Word', 'Lemma_word', 'POS'])\n","  return words_df"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2PbeWC1GTir","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1595423401083,"user_tz":-120,"elapsed":3682,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"f8a1f99f-6976-4bbb-d80d-a2ab4608711f"},"source":["tagme_words_df = lemmatize_pos_wordlist(tag_me_word_list)\n","algo_words_df = lemmatize_pos_wordlist(algo_10_files_word_list)\n","manual_words_df = lemmatize_pos_wordlist(manual_words_list)\n","algo_words_df.head()"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Raw Word</th>\n","      <th>Lemma_word</th>\n","      <th>POS</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Abfindung</td>\n","      <td>Abfindung</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Abs</td>\n","      <td>Abs</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Abschnitt</td>\n","      <td>Abschnitt</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>AfA</td>\n","      <td>AfA</td>\n","      <td>N</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Allgemeines</td>\n","      <td>Allgemeines</td>\n","      <td>N</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Raw Word   Lemma_word POS\n","0    Abfindung    Abfindung   N\n","1          Abs          Abs   N\n","2    Abschnitt    Abschnitt   N\n","3          AfA          AfA   N\n","4  Allgemeines  Allgemeines   N"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"5bCK30WvHmuI","colab_type":"text"},"source":["**Store Hyper and Hyponyms**"]},{"cell_type":"markdown","metadata":{"id":"rg9gx3RPdWmw","colab_type":"text"},"source":["For Manual List"]},{"cell_type":"code","metadata":{"id":"HQcfbBNodVkB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423410502,"user_tz":-120,"elapsed":541,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["def get_hyper_list(words_df):\n","  result_hyper_list = []\n","  for index, row in words_df.iterrows():\n","    result = OdeNet.word_info(row[\"Lemma_word\"])\n","    if result != '':\n","      hyper_obj = result[1]\n","      for item in hyper_obj:\n","        temp_list = list(item)\n","        sense = temp_list[0]\n","        hypers = temp_list[2]\n","        result_hyper_list.append([row[\"Raw Word\"], row[\"Lemma_word\"], row[\"POS\"], sense, hypers])\n","    else:\n","      result_hyper_list.append([row[\"Raw Word\"], row[\"Lemma_word\"], row[\"POS\"], '', ''])\n","\n","  df = pd.DataFrame(result_hyper_list, columns=[\"Raw_word\", \"Lemma_word\", \"POS\", \"Sense\", \"Hypernyms\"])\n","  return df"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ULA-a9Xhmtt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423412889,"user_tz":-120,"elapsed":454,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["def get_hypo_list(words_df):\n","  result_hypo_list = []\n","  for index, row in words_df.iterrows():\n","    result = OdeNet.word_info(row[\"Lemma_word\"])\n","    if result != '':\n","      hypo_obj = result[2]\n","      for item in hypo_obj:\n","        temp_list = list(item)\n","        sense = temp_list[0]\n","        hypos = list(temp_list[2])\n","        result_hypo_list.append([row[\"Raw Word\"], row[\"Lemma_word\"], row[\"POS\"], sense, hypos])\n","    else:\n","      result_hypo_list.append([row[\"Raw Word\"], row[\"Lemma_word\"], row[\"POS\"], '', ''])\n","\n","  df = pd.DataFrame(result_hypo_list, columns=[\"Raw_word\", \"Lemma_word\", \"POS\", \"Sense\", \"Hyponyms\"])\n","  return df"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"HSQfJf1ah5qS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423596538,"user_tz":-120,"elapsed":182767,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["manual_hypo_df = get_hypo_list(manual_words_df)\n","algo_hypo_df = get_hypo_list(algo_words_df)\n","tagme_hypo_df = get_hypo_list(tagme_words_df)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkwDFHS9hIlE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423829779,"user_tz":-120,"elapsed":185471,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["manual_hyper_df = get_hyper_list(manual_words_df)\n","algo_hyper_df = get_hyper_list(algo_words_df)\n","tagme_hyper_df = get_hyper_list(tagme_words_df)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U61hzH-9ldHS","colab_type":"text"},"source":["**Saving results to Excel**"]},{"cell_type":"code","metadata":{"id":"DlLUlkvAZBQa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423843646,"user_tz":-120,"elapsed":658,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["import pandas as pd"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwP_5zQjhPzR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423845797,"user_tz":-120,"elapsed":1519,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["# Create a Pandas Excel writer using XlsxWriter as the engine.\n","#writer = pd.ExcelWriter('/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/hyper_hypo_all.xlsx', engine='xlsxwriter')\n","with pd.ExcelWriter('/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/hyper_hypo_all.xlsx') as writer:\n","  # Write each dataframe to a different worksheet.\n","  manual_hyper_df.to_excel(writer, sheet_name='Manual - Hypernyms')\n","  manual_hypo_df.to_excel(writer, sheet_name='Manual - Hyponyms')\n","\n","  algo_hyper_df.to_excel(writer, sheet_name='Algo - Hyperyms')\n","  algo_hypo_df.to_excel(writer, sheet_name='Algo - Hyponyms')\n","\n","  tagme_hyper_df.to_excel(writer, sheet_name='Tagme - Hypernyms')\n","  tagme_hypo_df.to_excel(writer, sheet_name='Tagme - Hyponyms')"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"T3sOW2z4uH0F","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595423851033,"user_tz":-120,"elapsed":594,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}}},"source":["hyper_df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/hyper_hypo_all.xlsx', sheet_name='Manual - Hypernyms')\n","hypo_df = pd.read_excel('/content/drive/My Drive/Colab Notebooks/AutomaticOnt/odenet/hyper_hypo_all.xlsx', sheet_name='Manual - Hyponyms')"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"125XqFh8Y3yV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1595251412938,"user_tz":-120,"elapsed":445,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"5755f2f4-1322-453b-b892-3ccf415618c7"},"source":["hypo_df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Raw_word</th>\n","      <th>Lemma_word</th>\n","      <th>POS</th>\n","      <th>Sense</th>\n","      <th>Hyponyms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Rechnungslegung</td>\n","      <td>Rechnungslegung</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Verwaltungsanweisung</td>\n","      <td>Verwaltungsanweisung</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Freibetrags</td>\n","      <td>freibetrags</td>\n","      <td>ADV</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Rechtsbehelfsverfahrens</td>\n","      <td>Rechtsbehelfsverfahrens</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_1685-n</td>\n","      <td>['Regenerierung', 'Wiedergewinnung', 'Wiederve...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_1685-n</td>\n","      <td>['Eigennutz', 'zum privaten Nutzen', 'zum pers...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_1685-n</td>\n","      <td>['Hausgebrauch', 'Heimgebrauch']</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_1685-n</td>\n","      <td>['Verwendung', 'Auswertung', 'Nutzbarmachung',...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_5980-n</td>\n","      <td>['Regenerierung', 'Wiedergewinnung', 'Wiederve...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_5980-n</td>\n","      <td>['Anwendung', 'Programm', 'Applikation', 'Soft...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                           Hyponyms\n","0           0  ...                                                NaN\n","1           1  ...                                                NaN\n","2           2  ...                                                NaN\n","3           3  ...                                                NaN\n","4           4  ...  ['Regenerierung', 'Wiedergewinnung', 'Wiederve...\n","5           5  ...  ['Eigennutz', 'zum privaten Nutzen', 'zum pers...\n","6           6  ...                   ['Hausgebrauch', 'Heimgebrauch']\n","7           7  ...  ['Verwendung', 'Auswertung', 'Nutzbarmachung',...\n","8           8  ...  ['Regenerierung', 'Wiedergewinnung', 'Wiederve...\n","9           9  ...  ['Anwendung', 'Programm', 'Applikation', 'Soft...\n","\n","[10 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"iUX1lHmagefc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1595237430907,"user_tz":-120,"elapsed":654,"user":{"displayName":"Ravikant Tyagi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Git4N8GxzeRqoZVp3bkrF5657N1VC-kLyxNG8Or=s64","userId":"18219429055925147252"}},"outputId":"12d2b6e2-b986-4d17-99ca-37837e36401a"},"source":["hyper_df.head(10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Raw_word</th>\n","      <th>Lemma_word</th>\n","      <th>POS</th>\n","      <th>Sense</th>\n","      <th>Hypernyms</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Rechnungslegung</td>\n","      <td>Rechnungslegung</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Verwaltungsanweisung</td>\n","      <td>Verwaltungsanweisung</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Freibetrags</td>\n","      <td>freibetrags</td>\n","      <td>ADV</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Rechtsbehelfsverfahrens</td>\n","      <td>Rechtsbehelfsverfahrens</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_1685-n</td>\n","      <td>['Versuch', 'Vorsatz', 'Unternehmung']</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_5980-n</td>\n","      <td>['Versuch', 'Vorsatz', 'Unternehmung']</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_12037-n</td>\n","      <td>['Anordnung', 'Vorschrift', 'Anweisung', 'Instruktion']</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>Anwendung</td>\n","      <td>Anwendung</td>\n","      <td>N</td>\n","      <td>w8296_12037-n</td>\n","      <td>['Verwendung', 'Anwendung', 'Einsatz', 'Applikation']</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>Veranlagungszeitraum</td>\n","      <td>Veranlagungszeitraum</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>Ausgangswertgrenze</td>\n","      <td>Ausgangswertgrenze</td>\n","      <td>N</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                                Hypernyms\n","0           0  ...                                                      NaN\n","1           1  ...                                                      NaN\n","2           2  ...                                                      NaN\n","3           3  ...                                                      NaN\n","4           4  ...                   ['Versuch', 'Vorsatz', 'Unternehmung']\n","5           5  ...                   ['Versuch', 'Vorsatz', 'Unternehmung']\n","6           6  ...  ['Anordnung', 'Vorschrift', 'Anweisung', 'Instruktion']\n","7           7  ...    ['Verwendung', 'Anwendung', 'Einsatz', 'Applikation']\n","8           8  ...                                                      NaN\n","9           9  ...                                                      NaN\n","\n","[10 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"te6F5v37iNKE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LXgAo9xulkff","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}